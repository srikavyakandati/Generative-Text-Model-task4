# Text Generation Using GPT-2 and LSTM

## ğŸ“Œ Project Overview
This project implements **Text Generation** using:
1. **GPT-2 (Pretrained Transformer model)** â†’ Generates text based on a given prompt.
2. **LSTM (Recurrent Neural Network)** â†’ Trained on sample text to predict the next words in a sequence.

## ğŸš€ Features
- Uses **GPT-2 (Hugging Face)** for high-quality text generation.
- Implements **LSTM-based text generation** for custom datasets.
- Supports **user input prompts** to generate dynamic text.
- Includes **two separate scripts:**
  - `GPT.py` â†’ Uses GPT-2 for text generation.
  - `LSTM.py` â†’ Uses LSTM to generate text from a trained dataset.

## âš™ï¸ Installation
### **1ï¸âƒ£ Install Dependencies**
Ensure you have Python installed, then run:
```bash
pip install torch transformers tensorflow keras numpy matplotlib
```

## ğŸ“œ Usage
### **Run GPT-2 Model:**
```bash
python GPT.py
```
### **Run LSTM Model:**
```bash
python LSTM.py
```

### **Example Inputs & Outputs**
#### **ğŸ“ GPT-2 Input:**
```
Enter a text prompt: The future of artificial intelligence is
```
#### **ğŸ¯ GPT-2 Output:**
```
The future of artificial intelligence is evolving rapidly with advancements in deep learning and automation.
```

#### **ğŸ“ LSTM Input:**
```
Enter a seed text: Artificial intelligence is
```
#### **ğŸ¯ LSTM Output:**
```
Artificial intelligence is transforming industries by enabling automation, decision-making, and problem-solving.
```

## ğŸ“‚ File Structure
```
ğŸ“ text-generation/
â”œâ”€â”€ GPT.py  # GPT-2 text generation script
â”œâ”€â”€ LSTM.py  # LSTM text generation script
â”œâ”€â”€ README.md  # Project documentation
```
ğŸ‘¨â€ğŸ’» Developed by **Naveen K**